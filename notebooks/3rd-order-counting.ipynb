{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uniparse import Vocabulary\n",
    "from uniparse.dataprovider import batch_by_buckets\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.utils import class_weight\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from uniparse import Model, Vocabulary\n",
    "\n",
    "from uniparse.callbacks import ModelSaveCallback\n",
    "from uniparse.dataprovider import batch_by_buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# uniparse intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary()\n",
    "samples = vocab._read_conll(\"../data/ptb_conllu/train.conllu\", tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = random.choice(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[2][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract parent -> head -> modifier from treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "def count_3rd_order_sets(treebank_file, vocab):\n",
    "    samples = vocab._read_conll(treebank_file, tokenize=False)\n",
    "    counter = Counter()\n",
    "    for sample in samples:\n",
    "        extract_3O_paths(sample[3], sample[2], counter)\n",
    "        \n",
    "    factors = [{\"afactor\":\"%s->%s->%s\"%k, \"count\":v} for k,v in counter.items()]\n",
    "    df = pd.DataFrame.from_dict(factors).sort_values(\"count\", ascending=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extract_3O_paths(tree, tags, counter, root_token=\"<root>\"):    \n",
    "    for m, h in enumerate(tree):\n",
    "        p = tree[h]\n",
    "        parent_tag, head_tag, modifier_tag = [tags[i] if i >= 0 else tags[0] for i in [p, h, m]]\n",
    "        parent_tag, head_tag, modifier_tag = [t if t != 1 else root_token for t in [parent_tag, head_tag, modifier_tag]]\n",
    "        counter.update([(parent_tag, head_tag, modifier_tag)])\n",
    "\n",
    "def factor_to_str(t):\n",
    "    return \"->\".join(t)\n",
    "\n",
    "def get_factor_weights(dataframe):\n",
    "    dataset = []\n",
    "    factor_map = defaultdict(int)\n",
    "    for i, row in dataframe.iterrows():\n",
    "        factor = row[\"afactor\"]\n",
    "        factor_map[i] = factor\n",
    "        for _ in range(row[\"count\"]):\n",
    "            dataset.append(i)\n",
    "    classes = np.unique(dataset)\n",
    "    weights = class_weight.compute_class_weight(\"balanced\", classes, dataset)\n",
    "    return {factor_map[c]:w for c,w in zip(classes,weights)}\n",
    "\n",
    "def batch_to_weights(sample, factor_weights, tag_map):\n",
    "    (words, tags), (trees, labels) = sample\n",
    "    # batch :: (b, n)\n",
    "    output = np.zeros(trees.shape)\n",
    "    for b, tree in enumerate(trees):\n",
    "        for m, h in enumerate(tree):\n",
    "            p = tree[h]\n",
    "            parent_tag, head_tag, modifier_tag = [\n",
    "                tags[b, i] if i >= 0 else tags[b, 0]\n",
    "                for i in [p, h, m]\n",
    "            ]\n",
    "            factor_ = (parent_tag, head_tag, modifier_tag)\n",
    "            factor = [tag_map[e] for e in factor_]\n",
    "            factor = factor_to_str(factor)\n",
    "            try:\n",
    "                output[b, m] = factor_weights[factor]\n",
    "            except:\n",
    "                print(factor_)\n",
    "                raise\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptb_df = count_3rd_order_sets(\"../data/ptb_conllu/train.conllu\", vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ptb_df.shape)\n",
    "ptb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ptb_df[\"count\"] == 1).sum()/ptb_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptb_df.plot.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ptb_df[\"count\"] < 10\n",
    "x.astype(int).plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UD EWT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud_en_df = count_3rd_order_sets(\"../data/en_ewt-ud-train.conllu\", vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ud_en_df.shape)\n",
    "ud_en_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud_factor_weights = get_factor_weights(ud_en_df)\n",
    "ud_factor_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lets test it all out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary()\n",
    "vocab.fit(\"../data/en_ewt-ud-train.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vocab.tokenize_conll(\"../data/en_ewt-ud-train.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uniparse.dataprovider import batch_by_buckets\n",
    "X = batch_by_buckets(X, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, samples = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_weights = batch_to_weights(samples[0], ud_factor_weights, vocab._id2tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud_sample_weights = [batch_to_weights(samples[0], ud_factor_weights, vocab._id2tag) for sample in samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud_sample_weights[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hat = [(x+(w,),y)for (x,y), w in zip(X[1], ud_sample_weights)]\n",
    "words, tags, weights = X_hat[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.shape, weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lets do it from the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uniparse.models.dynet.syntax_att import Parser\n",
    "\n",
    "def train(train_file, dev_file, test_file, n_epochs, parameter_file, vocab_file, model_class):\n",
    "    \"\"\"Training procedure.\"\"\"\n",
    "    vocab = Vocabulary()\n",
    "    vocab = vocab.fit(train_file)\n",
    "    \n",
    "    # \"../data/en_ewt-ud-train.conllu\"\n",
    "    train_file_df = count_3rd_order_sets(train_file, vocab)\n",
    "    \n",
    "    # save vocab for reproducability later\n",
    "    if vocab_file:\n",
    "        print(\"> saving vocab to\", vocab_file)\n",
    "        vocab.save(vocab_file)\n",
    "\n",
    "    # prep data\n",
    "    print(\">> Loading in data\")\n",
    "    train_data = vocab.tokenize_conll(train_file)\n",
    "    dev_data = vocab.tokenize_conll(dev_file)\n",
    "    test_data = vocab.tokenize_conll(test_file)\n",
    "\n",
    "    train_batches = batch_by_buckets(train_data, batch_size=32, shuffle=True)\n",
    "    dev_batches = batch_by_buckets(dev_data, batch_size=32, shuffle=True)\n",
    "    test_batches = batch_by_buckets(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "    indicies, samples = train_batches\n",
    "    \n",
    "    factor_weights = get_factor_weights(train_file_df)\n",
    "\n",
    "    label_weights = [batch_to_weights(sample, factor_weights, vocab._id2tag) for sample in samples]\n",
    "    X_hat = [(x+(w,),y) for (x,y), w in zip(train_batches[1], label_weights)]\n",
    "    train_batches = (indicies, X_hat)\n",
    "\n",
    "    model = model_class(vocab)\n",
    "\n",
    "    save_callback = ModelSaveCallback(parameter_file)\n",
    "    callbacks = [save_callback]\n",
    "\n",
    "    # prep params\n",
    "    parser = Model(model, optimizer=\"adam\", vocab=vocab)\n",
    "\n",
    "    parser.train(train_batches, dev_file, dev_batches, epochs=n_epochs, callbacks=callbacks, verbose=True)\n",
    "    parser.load_from_file(parameter_file)\n",
    "\n",
    "    metrics = parser.evaluate(test_file, test_batches, delete_output=False)\n",
    "    test_UAS = metrics[\"nopunct_uas\"]\n",
    "    test_LAS = metrics[\"nopunct_las\"]\n",
    "\n",
    "    print(metrics)\n",
    "\n",
    "    print()\n",
    "    print(\">>> Model maxed on dev at epoch\", save_callback.best_epoch)\n",
    "    print(\">>> Test score:\", test_UAS, test_LAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> saving vocab to model.vocab\n",
      ">> Loading in data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1/30] arc 0.00, rel 0.71, loss 51.458: 100%|██████████| 451/451 [03:05<00:00,  2.44it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1][195s] 0.19691, 0.15200 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2/30] arc 0.00, rel 0.80, loss 64.475: 100%|██████████| 451/451 [02:54<00:00,  2.59it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2][183s] 0.44211, 0.37686 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[3/30] arc 0.00, rel 0.90, loss 2.980: 100%|██████████| 451/451 [02:46<00:00,  2.71it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3][174s] 0.42772, 0.37827 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[4/30] arc 0.41, rel 0.92, loss 3.281: 100%|██████████| 451/451 [02:45<00:00,  2.72it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4][174s] 0.69156, 0.56833 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[5/30] arc 0.19, rel 0.84, loss 49.354: 100%|██████████| 451/451 [02:48<00:00,  2.68it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5][176s] 0.76883, 0.67128 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[6/30] arc 0.51, rel 0.89, loss 7.437: 100%|██████████| 451/451 [02:53<00:00,  2.60it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6][181s] 0.80032, 0.70885 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[7/30] arc 0.66, rel 0.89, loss 1.830: 100%|██████████| 451/451 [02:47<00:00,  2.70it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7][175s] 0.80957, 0.73934 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[8/30] arc 0.48, rel 0.88, loss 23.262: 100%|██████████| 451/451 [02:45<00:00,  2.72it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8][173s] 0.81411, 0.74260 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[9/30] arc 0.95, rel 1.00, loss 0.139: 100%|██████████| 451/451 [02:45<00:00,  2.72it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9][174s] 0.82178, 0.75626 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/30] arc 0.70, rel 0.87, loss 2.994: 100%|██████████| 451/451 [02:45<00:00,  2.72it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10][173s] 0.82740, 0.74941 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11/30] arc 0.83, rel 0.95, loss 0.312: 100%|██████████| 451/451 [02:46<00:00,  2.71it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11][174s] 0.83457, 0.77600 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12/30] arc 0.75, rel 0.91, loss 2.192: 100%|██████████| 451/451 [02:46<00:00,  2.71it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12][174s] 0.83711, 0.77627 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[13/30] arc 0.71, rel 0.92, loss 1.563: 100%|██████████| 451/451 [02:45<00:00,  2.72it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13][173s] 0.84533, 0.78757 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14/30] arc 1.00, rel 1.00, loss 0.000: 100%|██████████| 451/451 [02:45<00:00,  2.72it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14][174s] 0.84678, 0.78462 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15/30] arc 0.82, rel 0.89, loss 0.390: 100%|██████████| 451/451 [02:46<00:00,  2.72it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15][174s] 0.84973, 0.78770 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16/30] arc 0.77, rel 0.90, loss 1.351: 100%|██████████| 451/451 [02:46<00:00,  2.71it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16][174s] 0.85290, 0.77319 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17/30] arc 0.91, rel 0.96, loss 0.243: 100%|██████████| 451/451 [02:46<00:00,  2.71it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17][174s] 0.85318, 0.79383 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18/30] arc 0.77, rel 0.93, loss 0.827: 100%|██████████| 451/451 [02:45<00:00,  2.72it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18][174s] 0.85018, 0.79397 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19/30] arc 0.90, rel 0.95, loss 0.312: 100%|██████████| 451/451 [02:46<00:00,  2.70it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19][174s] 0.85431, 0.78975 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20/30] arc 0.80, rel 0.91, loss 2.016: 100%|██████████| 451/451 [02:46<00:00,  2.71it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20][174s] 0.85413, 0.79469 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[21/30] arc 0.82, rel 0.92, loss 0.522: 100%|██████████| 451/451 [02:46<00:00,  2.71it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21][174s] 0.85563, 0.80268 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[22/30] arc 0.84, rel 0.95, loss 0.563: 100%|██████████| 451/451 [02:46<00:00,  2.71it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22][174s] 0.85313, 0.79873 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23/30] arc 0.83, rel 0.93, loss 0.716: 100%|██████████| 451/451 [02:46<00:00,  2.71it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23][174s] 0.85535, 0.80749 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[24/30] arc 0.86, rel 0.94, loss 0.412: 100%|██████████| 451/451 [02:46<00:00,  2.71it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24][174s] 0.85653, 0.80104 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[25/30] arc 0.80, rel 0.92, loss 1.658: 100%|██████████| 451/451 [02:46<00:00,  2.71it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25][174s] 0.86044, 0.80299 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[26/30] arc 0.90, rel 0.97, loss 0.109: 100%|██████████| 451/451 [02:46<00:00,  2.72it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26][174s] 0.85758, 0.80948 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[27/30] arc 0.81, rel 0.92, loss 0.592: 100%|██████████| 451/451 [02:45<00:00,  2.72it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27][173s] 0.86148, 0.81130 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[28/30] arc 0.85, rel 0.93, loss 0.467: 100%|██████████| 451/451 [02:46<00:00,  2.71it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28][174s] 0.86266, 0.81084 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[29/30] arc 0.95, rel 0.95, loss 0.040: 100%|██████████| 451/451 [02:46<00:00,  2.71it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29][174s] 0.86007, 0.80590 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[30/30] arc 0.89, rel 0.95, loss 18.990: 100%|██████████| 451/451 [02:48<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30][176s] 0.86384, 0.81298 \n",
      ">> Finished at epoch 30\n",
      ">> outputed predictions to /var/folders/0t/t8j215jn7z77m328dvr0dyfh3b9248/T/tmpob3j6o2w\n",
      "{'uas': 0.8500159387950271, 'las': 0.8057857825948358, 'nopunct_uas': 0.8615959531513466, 'nopunct_las': 0.8120129426240714}\n",
      "\n",
      ">>> Model maxed on dev at epoch 30\n",
      ">>> Test score: 0.8615959531513466 0.8120129426240714\n"
     ]
    }
   ],
   "source": [
    "TRAIN = \"../data/en_ewt-ud-train.conllu\"\n",
    "DEV = \"../data/en_ewt-ud-dev.conllu\"\n",
    "TEST = \"../data/en_ewt-ud-test.conllu\"\n",
    "train(TRAIN, DEV, TEST, 30, \"model.params\", \"model.vocab\", Parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
